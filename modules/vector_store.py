import os
import faiss
import numpy as np
import json
from dotenv import load_dotenv
from openai import OpenAI
from modules.text_processing import get_embedding
from rank_bm25 import BM25Okapi
from modules import vector_store
import re
import tiktoken

def normalize_text(text):
    return re.sub(r"\s+", "", text.strip())

encoding = tiktoken.get_encoding("cl100k_base")


# âœ… ê¸€ë¡œë²Œ ë³€ìˆ˜ë¡œ ë²¡í„° ì¸ë±ìŠ¤ ìºì‹±
FAISS_INDEX = None
BM25_CORPUS = []

# âœ… í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìì™€ ìˆ˜ì‹ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
def extract_numbers_and_formula(text):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìì™€ ê°„ë‹¨í•œ ìˆ˜ì‹ì„ ì¶”ì¶œ"""
    numbers = re.findall(r"\d+\.?\d*", text)
    return numbers

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# OpenAI API ì„¤ì •
client = OpenAI(api_key=OPENAI_API_KEY)

# FAISS ë²¡í„° ë°ì´í„° ì €ì¥ ê²½ë¡œ
FAISS_INDEX_PATH = "embeddings/faiss_index"
METADATA_PATH = "embeddings/metadata.json"

# FAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  í´ë” ìƒì„±
os.makedirs("embeddings", exist_ok=True)

# âœ… BM25 ê²€ìƒ‰ ì¸ë±ìŠ¤
bm25_corpus = []
bm25_index = None

# âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index():
    global FAISS_INDEX
    if not os.path.exists(FAISS_INDEX_PATH):
        print("âŒ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    FAISS_INDEX = faiss.read_index(FAISS_INDEX_PATH)
    return FAISS_INDEX

# BM25 ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_bm25_corpus():
    global BM25_CORPUS, bm25_index
    if not os.path.exists(METADATA_PATH):
        print("âŒ BM25 metadata.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    with open(METADATA_PATH, "r", encoding="utf-8") as f:
        BM25_CORPUS = json.load(f)
    bm25_index = BM25Okapi([doc.split() for doc in BM25_CORPUS])  # âœ… ì¸ë±ìŠ¤ë„ ì¬ìƒì„±
    return BM25_CORPUS

# âœ… FAISS + BM25 ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
def create_faiss_index(question_answer_pairs, general_chunks):
    global bm25_corpus, bm25_index

    print("ğŸ” FAISS ë° BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")

    qa_texts = [q["question"] for q in question_answer_pairs] if question_answer_pairs else []
    general_texts = general_chunks if general_chunks else []

    if not qa_texts and not general_texts:
        print("âš ï¸ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    dummy_text = "ê¸°ë³¸ ë”ë¯¸ ë°ì´í„°"
    dummy_embedding = get_embedding(dummy_text)

    qa_embeddings = np.array([get_embedding(text) for text in qa_texts], dtype=np.float32) if qa_texts else np.array([dummy_embedding], dtype=np.float32)
    general_embeddings = np.array([get_embedding(text) for text in general_texts], dtype=np.float32) if general_texts else np.empty((0, qa_embeddings.shape[1]), dtype=np.float32)

    qa_embeddings /= np.linalg.norm(qa_embeddings, axis=1, keepdims=True) + 1e-10
    if general_embeddings.shape[0] > 0:
        general_embeddings /= np.linalg.norm(general_embeddings, axis=1, keepdims=True) + 1e-10

    index = faiss.IndexFlatIP(qa_embeddings.shape[1])

    print(f"ğŸŸ¢ ë²¡í„° ì¶”ê°€ ì¤‘... (ì´ {qa_embeddings.shape[0] + general_embeddings.shape[0]}ê°œ)")
    all_embeddings = np.vstack((qa_embeddings, general_embeddings)) if general_embeddings.shape[0] > 0 else qa_embeddings
    index.add(all_embeddings)

    print("âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
    faiss.write_index(index, FAISS_INDEX_PATH)
    print("âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ!")

    bm25_corpus = qa_texts + general_texts if qa_texts or general_texts else [dummy_text]
    bm25_index = BM25Okapi([doc.split() for doc in bm25_corpus])

    print("âœ… BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

    return index, bm25_corpus

# âœ… FAISS + BM25 ê²€ìƒ‰ ì‹¤í–‰
def search_faiss(query, top_k=7, filter_type=None):
    global FAISS_INDEX, BM25_CORPUS
    if FAISS_INDEX is None:
        print("âŒ ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. main.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return []

    query_embedding = np.array([get_embedding(query)], dtype=np.float32)
    query_embedding /= np.linalg.norm(query_embedding)

    raw_k = top_k * 4
    distances, indices = FAISS_INDEX.search(query_embedding, raw_k)

    candidate_texts = []
    for dist, idx in zip(distances[0], indices[0]):
        if idx < len(BM25_CORPUS):
            similarity = 1 - dist
            if similarity < 0.3:
                continue
            text = BM25_CORPUS[idx]
            candidate_texts.append(text)

        # âœ… BM25 ì ìˆ˜ë¡œ ì¬ì •ë ¬ (normalize_text ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬)
    if bm25_index is not None:
        query_tokens = query.split()
        scores = bm25_index.get_scores(query_tokens)

        ranked = sorted(
            candidate_texts,
            key=lambda x: scores[BM25_CORPUS.index(x)],
            reverse=True
        )
    else:
        ranked = candidate_texts  # fallback

    results = [{"type": "text", "text": t} for t in ranked[:top_k]]
    return results


# âœ… ìˆ˜ì¹˜ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def execute_calculation(search_results):
    for res in search_results:
        if res["type"] == "qa":
            numbers = extract_numbers_and_formula(res["question"])
            if numbers:
                try:
                    numbers = [float(num) for num in numbers]
                    if "ìœ ë™ë¹„ìœ¨" in res["question"]:
                        result = (numbers[0] / numbers[1]) * 100
                        return f"ìœ ë™ë¹„ìœ¨ì€ {result:.2f}%ì…ë‹ˆë‹¤."
                except Exception as e:
                    return f"âŒ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    return None

# ë¬¸ì œ ê²€ìƒ‰
def find_similar_questions(query):
    search_results = vector_store.search_faiss(query, top_k=5)
    similar_questions = []

    for result in search_results:
        if "question" in result:
            similar_questions.append(result["question"])

    return similar_questions if similar_questions else ["ìœ ì‚¬í•œ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤."]

MAX_TEXT_TOKENS = 2000

def trim_text(text, max_tokens=MAX_TEXT_TOKENS):
    tokens = encoding.encode(text)
    return encoding.decode(tokens[:max_tokens])

# âœ… GPT ê¸°ë°˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(query, search_results):
    calculation_result = execute_calculation(search_results)
    if calculation_result:
        return calculation_result

    for result in search_results:
        if result["type"] == "text":
            result["text"] = trim_text(result["text"])

    valid_answers = []
    general_info = []
    limited_results = search_results

    for res in limited_results:
        if res["type"] == "qa" and res["answer"]:
            valid_answers.append(f"ë¬¸ì œ: {res['question']}\nì •ë‹µ: {res['answer']}")
        elif res["type"] == "text":
            general_info.append(res["text"])

    if not valid_answers and not general_info:
        return "âŒ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    context = "\n\n".join(valid_answers + general_info)
    context = context[:8000]

    prompt = f"""ë‹¹ì‹ ì€ ìŠ¤í¬ì¸ ê²½ì˜ê´€ë¦¬ì‚¬ ì‹œí—˜ì„ ë•ëŠ” AIì…ë‹ˆë‹¤.  
    ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{query}"  generate

    ğŸ“Œ **ì—­í• :**  
    - ìŠ¤í¬ì¸ ê²½ì˜ê´€ë¦¬ì‚¬ ì‹œí—˜ í•©ê²©ì„ ëª©í‘œë¡œ í•˜ëŠ” ìˆ˜í—˜ìƒì„ ì§€ì›í•©ë‹ˆë‹¤.  
    - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ì •í™•í•˜ê³  ë…¼ë¦¬ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    - ìˆ˜í—˜ìƒì´ ì›í• ê²½ìš° ë¬¸ì œë¥¼ ìƒì„±í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.  

    ğŸ“Œ **ì‘ë‹µ ê°€ì´ë“œ:**  
    - ë°˜ë“œì‹œ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.  
    - ê°œë…ì´ í•„ìš”í•œ ê²½ìš° ì„¤ëª…ì„ ë³´ì¶©í•˜ì„¸ìš”.  
    - ë¬¸ì¥ì€ ëª…í™•í•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    - ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ì •ë³´ê°€ ì…ë ¥ëœë‹¤ë©´ ì‚¬ì‹¤ëŒ€ë¡œ ë‹µë³€í•  ìˆ˜ ì—†ë‹¤ê³  ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    - ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•´ë„ ë©ë‹ˆë‹¤.

    ğŸ” **ì°¸ê³  ì •ë³´:**  
    {context}  

    âœï¸ **ë‹µë³€:**"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ìŠ¤í¬ì¸ ê²½ì˜ê´€ë¦¬ì‚¬ ì „ë¬¸ê°€ì´ë©°, ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500
    )

    return response.choices[0].message.content
